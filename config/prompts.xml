<?xml version="1.0" encoding="UTF-8"?>
<root xmlns="http://nlweb.ai/base"
      xmlns:so="http://www.schema.org/"
      xmlns:rdfs="http://www.w3.org/rdfs">

<!-- This file has all the prompts that are used in the process of running a query. -->

  <Item>

    <Prompt ref="DetectIrrelevantQueryPrompt">
      <promptString>
        The user is querying the site {request.site} which has information about {site.itemType}s.
        Is the site utterly completely irrelevant to the user's query? 
        The question is not whether this is the best site for answering the query, 
        but if there is nothing on the site that is likely to be relevant for the query. 
        If the site is irrelevant, add an explanation for why it is irrelevant. Otherwise, leave that field blank.
        The user query is: '{request.query}'
      </promptString>
      <returnStruc>
        {
          "site_is_irrelevant_to_query": "True or False",
          "explanation_for_irrelevance": "Explanation for why the site is irrelevant"
        }
      </returnStruc>
    </Prompt>

    <Prompt ref="PrevQueryDecontextualizer">
      <promptString>
        The user is querying the site {request.site} which has {site.itemType}s.
        Rewrite the query, incorporating the context of the previous queries and answers.
        Keep the decontextualized query short and do not reference the site. 

        If the query very clearly does not reference earlier queries, 
        don't change the query. Err on the side of incorporating the context of the 
        previous queries. If you are not sure whether this is a brand new query, 
        or follow up, it is likely a follow up. Try your best to incorporate the 
        context from the previous queries.

        The user's query is: {request.rawQuery}. 
        Previous queries were: {request.previousQueries}.
       <!-- Previous answers (title + url)provided were: {request.prevAnswers}. -->
      </promptString>
      <returnStruc>
        {
          "requires_decontextualization": "True or False",
          "decontextualized_query": "The rewritten query, if decontextualization is required"
        }
      </returnStruc>
    </Prompt>

    <Prompt ref="DecontextualizeContextPrompt">
      <promptString>
        The user is asking the following question: '{request.rawQuery}' in the context of 
        the an item with the following description: {request.contextDescription}. 
        Previous answers provided were: {request.prevAnswers}.
        Rewrite the query to decontextualize it so that it can be answered 
        without reference to earlier queries, previous answers, or to the item description.
      </promptString>
      <returnStruc>
        {
          "decontextualized_query": "The rewritten query"
        }
      </returnStruc>
    </Prompt>

    <Prompt ref="FullDecontextualizePrompt">
      <promptString>
        The user is asking the following question: '{request.rawQuery}' in the context of 
        the an item with the following description: {request.contextDescription}. 
        Previous queries from the user were: {request.previousQueries}.
        Previous answers provided were: {request.prevAnswers}.
        Rewrite the query to decontextualize it so that it can be answered 
        without reference to earlier queries, previous answers, or to the item description.
      </promptString>
      <returnStruc>
        {
          "decontextualized_query": "The rewritten query"
        }
      </returnStruc>
    </Prompt>

    <Prompt ref="DetectMemoryRequestPrompt">
      <promptString>
        Analyze the following statement from the user. 
        Is the user asking you to remember, that may be relevant to not just this query, but also future queries? 
        If so, what is the user asking us to remember?
        The user should be explicitly asking you to remember something for future queries, 
        not just expressing a requirement for the current query.
        Keep the memory_request field short and do not reference the user or site.
        The user's query is: {request.rawQuery}.
      </promptString>
      <returnStruc>
        {
          "is_memory_request": "True or False",
          "memory_request": "The memory request, if any"
        }
      </returnStruc>
    </Prompt>

    <Prompt ref="QueryRewrite">
      <promptString>
        You are helping to rewrite a complex search query into simpler keyword queries for a traditional keyword-based search engine.
        The search engine works best with short, focused queries containing important keywords.
        
        Take the following query and break it down into up to 5 simpler search queries.
        Each query should:
        - Contain no more than 3 words
        - Focus on the most important keywords and concepts
        - Be diverse to cover different aspects of the original query
        - Use only essential nouns, adjectives, or product terms
        - Avoid common words like "for", "the", "some", "are", "that", "would", "be"
        
        For example:
        - "what are some options for plates that would be appropriate for serving vegetables" → ["vegetable plates", "serving plates", "dinner plates", "salad plates", "ceramic plates"]
        - "looking for a tea pot that can brew green tea" → ["tea pot", "green tea", "teapot ceramic", "japanese teapot", "brewing pot"]
        
        The original query is: {request.query}
      </promptString>
      <returnStruc>
        {
          "rewritten_queries": ["query1", "query2", "query3", "query4", "query5"],
          "query_count": "Number of queries generated (1-5)"
        }
      </returnStruc>
    </Prompt>

    <Prompt ref="DetectMultiItemTypeQueryPrompt">
      <promptString>
        Analyze the following query from the user.
        Is the user asking for only one kind of item or are they asking for multiple kinds of items.
        If they are asking for multiple kinds of items, construct indepenent queries for each of the 
        kinds of items, separated by semicolons. The user's query is: {request.query}.
      </promptString>
      <returnStruc>
        {
          "single_item_type_query": "True or False",
          "item_queries": "Separate queries for each of the kinds of items, separated by commas"
        }
      </returnStruc>
    </Prompt>

    <Prompt ref="DetectItemTypePrompt">
      <promptString>
        What is the kind of item the query is likely seeking with this query: {request.query}
        
        Common item types include:
        - Recipe (for cooking, food preparation)
        - Movie (for films, cinema)
        - Product (for items to purchase)
        - Restaurant (for dining establishments)
        - Statistics (for demographic data, population statistics, economic indicators about places like counties, cities, states)
        - Item (for general items that don't fit other categories)
        
        If the query is asking about statistical data like population, median income, demographics, or economic indicators for geographic locations, return "Statistics".
      </promptString>
      <returnStruc>
        {
          "item_type": ""
        }
      </returnStruc>
    </Prompt>

    <Prompt ref="DetectQueryTypePrompt">
      <promptString>
        Analyze the following query from the user. 
        Is the user asking for a list of {site.itemType} that match a certain description or are they asking for the
        details of a particular {site.itemType}?
        If the user for the details of a particular {site.itemType}, what is the name of the {site.itemType} and what
        details are they asking for? The user's query is: {request.query}.
      </promptString>
      <returnStruc>
        {
          "item_details_query": "True or False",
          "item_title": "The title of the item type, if any",
          "details_being_asked": "what details the user is asking for"
        }
      </returnStruc>
    </Prompt>

    <Prompt ref="RankingPromptWithExplanation">
      <promptString>
        Assign a score between 0 and 100 to the following {site.itemType}
        based on how relevant it is to the user's question. Use your knowledge from other sources, about the item, to make a judgement.
        Provide a short description of the item that is relevant to the user's question, without mentioning the user's question.
        Provide an explanation of the relevance of the item to the user's question, without mentioning the user's question or the score or explicitly mentioning the term relevance.
        If the score is below 75, in the description, include the reason why it is still relevant.
        The user's question is: {request.query}. The item's description is {item.description}
      </promptString>
      <returnStruc>
        {
          "score": "integer between 0 and 100",
          "description": "short description of the item",
          "explanation": "explanation of the relevance of the item to the user's question"
        }
      </returnStruc>
    </Prompt>

    <Prompt ref="RankingPrompt">
      <promptString>
        Assign a score between 0 and 100 to the following item
        based on how relevant it is to the user's question. Use your knowledge from other sources, about the item, to make a judgement. 
        
        CRITICAL: If the user's question mentions a specific date (like "today", "tomorrow", "Monday", "2025-08-02", etc.) or meal type (like "lunch", "dinner", "breakfast"), you MUST check the item's menuDatePublished and menuMeal fields. Only give a high score (above 70) if the item matches BOTH the EXACT date and meal type requested. If the dates or meal types don't match EXACTLY, give a very low score (below 20). For example, if the user asks for "2025-08-02" and the item has menuDatePublished "2025-07-27", this is a complete mismatch and should get a score of 0-10.
        
        ALSO CRITICAL: If the user's question mentions a specific dining section, use intelligent scoring:
        
        **Section Scoring:**
        - Exact section name matches: score 90-100
        - Semantic section matches (similar concepts): score 70-89
        - Related sections: score 50-69
        - Wrong sections: score 0-49
        
        **Meal Type Scoring:**
        - If user specifies a meal type and item matches: no penalty
        - If user specifies a meal type and item doesn't match: reduce score by 30-50 points
        - If user doesn't specify meal type: no penalty
        
        **Dining Hall Scoring:**
        - If user specifies a dining hall and item matches: no penalty
        - If user specifies a dining hall and item doesn't match: reduce score by 40-60 points (stronger penalty)
        - If user doesn't specify dining hall: no penalty
        - CRITICAL: If user mentions "Gordon's" or "Gordon Avenue Market", ONLY include items where menuHall="gordon-avenue-market"
        
        If the score is above 50, provide a short description of the item highlighting the relevance to the user's question, without mentioning the user's question.
        Provide an explanation of the relevance of the item to the user's question, without mentioning the user's question or the score or explicitly mentioning the term relevance.
        If the score is below 75, in the description, include the reason why it is still relevant.
        
        The user's question is: \"{request.query}\". The item's description in schema.org format is \"{item.description}\".
      </promptString>
      <returnStruc>
        {
          "score": "integer between 0 and 100",
          "description": "short description of the item"
        }
      </returnStruc>
    </Prompt>

    <Prompt ref="RankingPromptForGenerate">
      <promptString>
        Assign a score between 0 and 100 to the following item
        based on useful it might be to answering the user's question. 
        
        CRITICAL: If the user's question mentions a specific date (like "today", "tomorrow", "Monday", "2025-08-02", etc.) or meal type (like "lunch", "dinner", "breakfast"), you MUST check the item's menuDatePublished and menuMeal fields. Only give a high score (above 70) if the item matches BOTH the EXACT date and meal type requested. If the dates or meal types don't match EXACTLY, give a very low score (below 20). For example, if the user asks for "2025-08-02" and the item has menuDatePublished "2025-07-27", this is a complete mismatch and should get a score of 0-10.
        
        ALSO CRITICAL: If the user's question mentions a specific dining section, use intelligent scoring:
        
        **Section Scoring:**
        - Exact section name matches: score 90-100
        - Semantic section matches (similar concepts): score 70-89
        - Related sections: score 50-69
        - Wrong sections: score 0-49
        
        **Meal Type Scoring:**
        - If user specifies a meal type and item matches: no penalty
        - If user specifies a meal type and item doesn't match: reduce score by 30-50 points
        - If user doesn't specify meal type: no penalty
        
        **Dining Hall Scoring:**
        - If user specifies a dining hall and item matches: no penalty
        - If user specifies a dining hall and item doesn't match: reduce score by 40-60 points (stronger penalty)
        - If user doesn't specify dining hall: no penalty
        - CRITICAL: If user mentions "Gordon's" or "Gordon Avenue Market", ONLY include items where menuHall="gordon-avenue-market"
        - CRITICAL: If user mentions "Four Lakes Market" or "Four Lakes", ONLY include items where menuHall="four-lakes-market"
        
        **CRITICAL SECTION LOGIC:**
        - If user mentions ONLY a dining hall (like "Four Lakes Market", "Gordon's") without specifying a section, DO NOT default to any specific section. Include items from ALL sections within that dining hall.
        - If user mentions a specific section (like "Fired Up", "1849", "Buona Cucina"), ONLY include items from that exact section or semantically identical sections.
        - Do NOT assume "Gordon's" means "Fired Up" - "Gordon's" refers to the dining hall, not any specific section.
        - Do NOT assume "Four Lakes Market" means any specific section - include all sections unless explicitly specified.
        - For dining hall queries without specific sections, be more lenient in scoring - items from the correct dining hall should get higher scores even if not from a specific section.
        - For meal planning queries, be more lenient and consider items from all relevant sections within the specified dining hall.
        - When the user mentions a dining hall (like "Gordon's", "Four Lakes"), prioritize items from that dining hall even if they don't mention specific sections.
        - For generic queries like "dinner items", consider items from all dining halls unless a specific dining hall was mentioned in the conversation context.
        
        The user's question is: \"{request.query}\".
        The item in schema.org format is: \"{item.description}\". Include a 
        short description of the item in the description field.
      </promptString>
      <returnStruc>
        {
          "score" : "integer between 0 and 100",
          "description" : "short description of the item"
        }
      </returnStruc>
    </Prompt>
     
    
    <Prompt ref="SynthesizePromptForGenerate">
      <promptString>
        Given the following items, synthesize an answer to the user's question. 
        
        **CRITICAL COMPREHENSIVENESS RULE:** If the user asks "What is offered", "What's available", "What's on the menu", "Show me the breakfast options", "Draft me a balanced lunch", "Draft me a meal plan", "List out", or similar questions asking for a comprehensive list or meal planning, you MUST include ALL relevant items that match their criteria. Be extremely thorough and do not miss any items that fit the user's request. This includes main dishes, side items, desserts, and any other offerings. Treat "What's offered?" and "List out..." as equivalent requests for comprehensive information.
        
        **CRITICAL MEAL PLANNING RULE:** When the user asks for meal plans (breakfast, lunch, dinner), you MUST search through ALL available meals and sections. Do NOT limit yourself to only breakfast items when asked for a full day meal plan. Look for lunch and dinner items in the provided data and include them appropriately.
        
        For meal planning queries (like "draft a meal plan", "nutritious meal", "protein goal"), consider items from ALL relevant sections and meals that could work together, and always include the key macronutrients (calories, protein, fat, carbs) for each suggested item. Remember that a "meal" refers to a combination of multiple food items, not individual items.
        
        For other types of questions, you do not need to include all the items, but you should include the most relevant ones.
        
        CRITICAL: If the user's question mentions a specific date (like "today", "tomorrow", "Monday", "2025-08-02", etc.) or meal type (like "lunch", "dinner", "breakfast"), prioritize items that match BOTH the EXACT date and meal type requested. Only include items that are relevant to the specific date and meal type mentioned in the user's question. For example, if the user asks for "2025-08-02" and the item has menuDatePublished "2025-07-27", this is a complete mismatch and should NOT be included in the response. When multiple days of data are available, be extremely precise about date matching - do not mix items from different days unless explicitly requested.
        
        ALSO CRITICAL: If the user's question mentions a specific dining hall or section, check the item's menuHall and menuSectionName fields. Use intelligent matching:
        
        **Section Matching Strategy:**
        - Look for semantic similarity in section names (e.g., "Fired Up", "Gordon Fired Up", "Four Lakes Fired Up" all refer to similar concepts)
        - Consider the dining hall context (Gordon Avenue Market vs Four Lakes Market)
        - If the user mentions a specific dining hall, prioritize items from that hall
        - If the user doesn't specify a dining hall, consider items from all halls that have similar sections
        - CRITICAL: Do NOT assume "Gordon's" means "Fired Up" - "Gordon's" refers to the dining hall, not any specific section. Only match sections that the user explicitly mentions or that are semantically similar to what they ask for
        - CRITICAL: If user specifies a specific section (like "Fired Up", "1849", "Buona Cucina"), ONLY include items from that exact section or semantically identical sections
        - CRITICAL: If user mentions only a dining hall (like "Four Lakes Market", "Gordon's") without specifying a section, include items from ALL sections within that dining hall for the specified meal/date
        
        **Meal Type Priority:**
        - If the user specifies a meal type (lunch, dinner, breakfast), ONLY include items from that meal
        - If the user doesn't specify a meal type, you may include items from multiple meals but clearly indicate which meal each item is from
        
        **Scoring Logic:**
        - Exact section name matches get highest priority
        - Semantic section matches get high priority
        - Items from the correct dining hall get priority over items from other halls
        - Items from the specified meal type get priority over other meals
        
        IMPORTANT: Do NOT return "I couldn't find relevant information" unless you have thoroughly checked all items and confirmed none match the user's criteria. If you find ANY items that could be relevant, include them. For meal planning, always suggest combinations of multiple items rather than saying only one item is available.
        
        **CONTEXT PRESERVATION:** When the user mentions a specific dining hall (like "Gordon's", "Four Lakes"), maintain that context throughout your response. If they ask for "dinner items" after mentioning a dining hall, focus on that dining hall's dinner items. If they ask for meal plans, consider items from that specific dining hall across all meals.
        

        
        If you encounter any errors or issues processing the request, try to provide a helpful response based on the information you can access, rather than returning an error message.
        
        The user's question is: {request.query}.
        The items are: {request.answers}.
      </promptString>
      <returnStruc>
        {
          "answer" : "string"
        }
      </returnStruc>
    </Prompt>

     <Prompt ref="SummarizeResultsPrompt">
      <promptString>
        Given the following items, summarize the results as an answer to the user's question. `
        The user's question is: {request.query}. 
        The items are: {request.answers}.
      </promptString>
      <returnStruc>
        {
          "summary" : "string"
        }
      </returnStruc>
    </Prompt>

    <Prompt ref="DescriptionPromptForGenerate">
      <promptString>
        The item with the following description is used to answer the user's question. 
        Provide ONLY a concise nutrition summary in this format:
        "Calories: X, Protein: Xg, Fat: Xg, Carbs: Xg"
        
        If nutrition data is not available, say "Nutrition info not available"
        Do NOT provide any other description, analysis, or commentary.
        Keep it brief and factual.
        
        The user's question is: {request.query}.
        The overall answer is: {request.answer}.
        The item is: {item.description}.
      </promptString>
      <returnStruc>
        {
          "description" : "string"
        }
      </returnStruc>
    </Prompt>

    <Prompt ref="ItemMatchingPrompt">
      <promptString>
        The user is looking for some details about: {request.item_name}
        
        Assign a score between 0 and 100 for whether the following item description
        matches what the user is looking for. A score of 100 means this is exactly
        the item they want, 0 means it's completely unrelated.

        If the score is above 75, also extract the details that the user is looking for.
        Include only the details that the user is explicitly asking for.

        The details requested are: {request.details_requested}. 
        
        Item description: {item.description}
      </promptString>
      <returnStruc>
        {
          "score": "integer between 0 and 100",
          "item_details": "the specific details requested by the user"
        }
      </returnStruc>
    </Prompt>

    <Prompt ref="ExtractItemDetailsPrompt">
      <promptString>
        The user is requesting specific details about an item they previously saw.
        
        The user's query: {request.query}
        The details requested: {request.details_requested}
        
        From the following item description, extract ONLY the specific details the user is asking for.
        Be comprehensive but focused - include all relevant information for what they asked, but don't include unrelated details.
        
        For example:
        - If they ask for "ingredients", include the full ingredients list
        - If they ask for "price", include price and any pricing variations
        - If they ask for "nutrition", include all nutritional information
        - If they ask for "description", provide a clear summary of the item
        - If they ask for "instructions" or "how to make", include step-by-step instructions
        
        Item description: {item.description}
      </promptString>
      <returnStruc>
        {
          "item_name": "the name of the item",
          "requested_details": "the specific details requested by the user, formatted clearly",
          "additional_context": "any important context about these details (optional)"
        }
      </returnStruc>
    </Prompt>


    <Prompt ref="FindItemPrompt"> 
      <promptString>
        The user is looking for for an item named / described as: {item.name}
        Assign a score between 0 and 100 for whether the following item description
        matches the items the user is looking for. A score of 100 means this is exactly
        the item they want, 0 means it's completely unrelated.
        Item description: {item.description}
      </promptString>
      <returnStruc>
        {
          "score": "integer between 0 and 100"
        }
      </returnStruc>
    </Prompt>

    
    
    <Prompt ref="CompareItemsPrompt">
      <promptString>
      The user is looking for a comparison between two items with the following descriptions:
      Item 1: {request.item1_description} 
      vs
      Item 2: {request.item2_description} 
      The user is asking for the following details: {request.details_requested}
      Provide a comparison of the two items, highlighting the differences and similarities.
      The user's query is: {request.query}.
      </promptString>
      <returnStruc>
        {
          "comparison": "Comparison of the two items"
        }
      </returnStruc>
    </Prompt>

    <Prompt ref="CompareItemDetailsPrompt">
      <promptString>
      The user is looking for a comparison between two items with the following descriptions:
      Item 1: {request.item1_description} 
      and
      Item 2: {request.item2_description} 
      The user is asking for the following details: {request.details_requested}
      Provide a comparison of the two items, highlighting the differences and similarities.
      The user's query is: {request.query}.
      </promptString>
      <returnStruc>
        {
          "comparison": "Comparison of the two items"
        }
      </returnStruc>
    </Prompt>

  </Item>

  <Recipe>
    <Prompt ref="DetectMemoryRequestPrompt">
      <promptString>
        Analyze the following statement from the user. 
        Is the user asking you to remember a dietary constraint, that may be relevant
        to not just this query, but also future queries? For example, the user may say
        that they are vegetarian or observe kosher or halal or specify an allergy such
        as gluten or lactose intolerance.
        If so, what is the user asking us to remember?
        The user should be explicitly asking you to remember something for future queries, 
        not just expressing a requirement for the current query.
         Keep the memory_request field short and do not reference the user or site.
        The user's query is: {request.rawQuery}.
      </promptString>
      <returnStruc>
        {
          "is_memory_request": "True or False",
          "memory_request": "The memory request, if any"
        }
      </returnStruc>
     </Prompt>



 <Prompt ref="ItemMatchingPrompt">
      <promptString>
        The user is looking for some details about: {request.item_name} and the users query is: {request.query}.
        Assign a score between 0 and 100 for whether the following item description
        matches what the user is looking for. A score of 100 means this is exactly
        the item they want, 0 means it's completely unrelated.

        If the score is above 75, also extract the details that the user is looking for.
        Include only the details that the user is explicitly asking for.
        If they asked for ingredients, provide only the ingredients list.
        If they asked for instructions, provide only the cooking steps.
        If they asked for nutrition info, provide only nutritional details.
        
        Be direct and specific - extract exactly what they asked for from the data.
        The details requested are: {request.details_requested}. 
        
        Item description: {item.description}
      </promptString>
      <returnStruc>
        {
          "score": "integer between 0 and 100",
          "item_details": "the specific details requested by the user"
        }
      </returnStruc>
    </Prompt>

        <Prompt ref="RankingPrompt">
      <promptString>
        Assign a score between 0 and 100 to the following item
        based on how relevant it is to the user's question. Use your knowledge from other sources, about the item, to make a judgement. 
        
        CRITICAL: If the user's question mentions a specific date (like "today", "tomorrow", "Monday", "2025-08-02", etc.) or meal type (like "lunch", "dinner", "breakfast"), you MUST check the item's menuDatePublished and menuMeal fields. Only give a high score (above 70) if the item matches BOTH the EXACT date and meal type requested. If the dates or meal types don't match EXACTLY, give a very low score (below 20). For example, if the user asks for "2025-08-02" and the item has menuDatePublished "2025-07-27", this is a complete mismatch and should get a score of 0-10.
        
        ALSO CRITICAL: If the user's question mentions a specific dining section (like "Gordon's Fired Up", "Fired Up", etc.), check the item's menuSectionName field. Items from wrong sections should get lower scores.
        
        If the score is above 50, provide a short description of the item highlighting the relevance to the user's question, without mentioning the user's question.
        Provide an explanation of the relevance of the item to the user's question, without mentioning the user's question or the score or explicitly mentioning the term relevance.
        If the score is below 75, in the description, include the reason why it is still relevant.

        If the item is not a Recipe, it should get a substantially lower score.

        The user's question is: \"{request.query}\". The item's description in schema.org format is \"{item.description}\".
      </promptString>
      <returnStruc>
        {
          "score": "integer between 0 and 100",
          "description": "short description of the item"
        }
      </returnStruc>
    </Prompt>

  </Recipe>

  <RealEstate>
    <Prompt ref="RequiredInfoPrompt">
      <promptString>
        Answering the user's query requires the location and price range.
        Do you have this information from this
        query or the previous queries or the context or memory about the user? 
        The user's query is: {request.query}. The previous queries are: {request.previousQueries}.
      </promptString>
      <returnStruc>
        {
          "required_info_found": "True or False",
          "user_question": "Question to ask the user for the required information"
        }
      </returnStruc>
    </Prompt>
  </RealEstate>

  <Item>
    <Prompt ref="EnsembleBasePrompt">
      <promptString>
        Based on the user's request: "{request.query}"

        I searched for: {ensemble.queries}

        Here are the search results:
        {ensemble.results}

        Please create a cohesive recommendation that addresses the user's request.
      </promptString>
      <returnStruc>
        {
          "theme": "Brief description of the overall recommendation theme",
          "items": [
            {
              "category": "Category name (e.g., Appetizer, Museum, Footwear)",
              "name": "Specific item name", 
              "description": "Detailed description",
              "why_recommended": "Why this item fits the request",
              "details": {}
            }
          ],
          "overall_tips": ["Any general tips or considerations"]
        }
      </returnStruc>
    </Prompt>

    <Prompt ref="EnsembleMealPlanningPrompt">
      <promptString>
        Based on the user's request: "{request.query}"

        I searched for: {ensemble.queries}

        Here are the search results:
        {ensemble.results}

        Create a complete meal recommendation with:
        1. Appetizer/Starter
        2. Main Course
        3. Dessert

        For each course, provide:
        - Name of the dish
        - Brief description
        - Why it complements the other courses
        - Any relevant details (prep time, difficulty, dietary info)
        - URL of the item

        Include the URL of the item in the 'url' field of the structured output.

        Ensure the courses work well together in terms of flavors, textures, and overall dining experience.
      </promptString>
      <returnStruc>
        {
          "theme": "Brief description of the meal theme",
          "items": [
            {
              "category": "Course type (Appetizer, Main Course, or Dessert)",
              "name": "Specific dish name",
              "description": "Detailed description of the dish",
              "why_recommended": "Why this dish fits the meal",
              "url": "URL of the item",
              "details": {
                "prep_time": "Preparation time",
                "difficulty": "Easy/Medium/Hard",
                "dietary_info": "Any dietary considerations"
              }
            }
          ],
          "overall_tips": ["Tips for preparing or serving the meal"]
        }
      </returnStruc>
    </Prompt>

    <Prompt ref="EnsembleTravelItineraryPrompt">
      <promptString>
        Based on the user's request: "{request.query}"

        I searched for: {ensemble.queries}

        Here are the search results:
        {ensemble.results}

        Create a travel itinerary recommendation with:
        1. Attractions/Museums to visit
        2. Nearby restaurants for meals
        3. Suggested order/timing

        For each recommendation, provide:
        - Name and location
        - Why it's worth visiting
        - Time needed
        - How it connects to other recommendations
        - Any practical tips (hours, tickets, reservations)
        - URL of the item

        Include the URL of the item in the 'url' field of the structured output.
      </promptString>
      <returnStruc>
        {
          "theme": "Brief description of the itinerary theme",
          "items": [
            {
              "category": "Category (Attraction, Museum, Restaurant, etc.)",
              "name": "Specific venue name",
              "description": "Detailed description",
              "why_recommended": "Why this fits the itinerary",
              "url": "URL of the item",
              "details": {
                "location": "Address or area",
                "time_needed": "Estimated duration",
                "best_time": "Recommended time to visit",
                "tickets": "Ticket information if applicable",
                "reservations": "Reservation requirements"
              }
            }
          ],
          "overall_tips": ["General tips for the itinerary"]
        }
      </returnStruc>
    </Prompt>

    <Prompt ref="EnsembleOutfitPrompt">
      <promptString>
        Based on the user's request: "{request.query}"

        I searched for: {ensemble.queries}

        Here are the search results, each with a URL:
        {ensemble.results}

        Create a complete outfit recommendation with:
        1. Essential items (footwear, jacket, base layers)
        2. Accessories
        3. Additional considerations

        For each item, provide:
        - Specific type recommended
        - Why it's suitable for the conditions
        - Key features to look for
        - URL of the item
        - Any alternatives

        Include the URL of the item in the 'url' field of the structured output.

        Consider weather, activity level, and safety.
      </promptString>
      <returnStruc>
        {
          "theme": "Brief description of the outfit purpose",
          "items": [
            {
              "category": "Category (Footwear, Jacket, Accessories, etc.)",
              "name": "Specific item type",
              "description": "Detailed description",
              "why_recommended": "Why this item is suitable",
              "url": "URL of the item",
              "details": {
                "key_features": "Important features to look for",
                "alternatives": "Alternative options",
                "weather_suitability": "How it handles weather conditions",
                "activity_level": "Suitable activity level"
              }
            }
          ],
          "overall_tips": ["General outfit tips and safety considerations"]
        }
      </returnStruc>
    </Prompt>

    <Prompt ref="EnsembleGenericPrompt">
      <promptString>
        Based on the user's request: "{request.query}"

        I searched for: {ensemble.queries}

        Here are the search results:
        {ensemble.results}

        Create a cohesive set of recommendations that work well together.
        For each item, explain why it's recommended and how it complements the other items.
        Include the URL of the item in the 'url' field of the structured output.
      </promptString>
      <returnStruc>
        {
          "theme": "Brief description of the recommendation theme",
          "items": [
            {
              "category": "Item category",
              "name": "Specific item name",
              "description": "Detailed description",
              "why_recommended": "Why this item fits the request",
              "url": "URL of the item",
              "details": {}
            }
          ],
          "overall_tips": ["General tips or considerations"]
        }
      </returnStruc>
    </Prompt>

    <Prompt ref="EnsembleItemRankingPrompt">
      <promptString>
        Given the user's query: "{request.query}"

        And this item:
        Name: {item.name}
        Type: {item.type}
        Description: {item.description}

        Rate how relevant this item is for answering the user's query on a scale of 0-100.
        Consider:
        - Does this item directly address what the user is looking for?
        - Is it the right type of item (e.g., restaurant vs attraction)?
        - Does it match any specific criteria mentioned in the query?

        Provide your response as a JSON object with a 'score' field containing the relevance score.
      </promptString>
      <returnStruc>
        {
          "score": "integer between 0 and 100"
        }
      </returnStruc>
    </Prompt>
  </Item>

  <Statistics>
    <Prompt ref="DetectItemTypePrompt">
      <promptString>
        What is the kind of item the query is likely seeking with this query: {request.query}
        
        Common item types include:
        - Recipe (for cooking, food preparation)
        - Movie (for films, cinema)
        - Product (for items to purchase)
        - Restaurant (for dining establishments)
        - Statistics (for demographic data, population statistics, economic indicators about places like counties, cities, states)
        - Item (for general items that don't fit other categories)
        
        If the query is asking about statistical data like population, median income, demographics, or economic indicators for geographic locations, return "Statistics".
      </promptString>
      <returnStruc>
        {
          "item_type": ""
        }
      </returnStruc>
    </Prompt>
    
    <Prompt ref="RankingPrompt">
      <promptString>
        Assign a score between 0 and 100 to the following statistical data point
        based on how relevant it is to the user's question about demographic or economic indicators.
        The user's question is: "{request.query}". 
        The data point is: "{item.description}".
      </promptString>
      <returnStruc>
        {
          "score": "integer between 0 and 100",
          "description": "short description of the data point"
        }
      </returnStruc>
    </Prompt>
  </Statistics>

</root>
